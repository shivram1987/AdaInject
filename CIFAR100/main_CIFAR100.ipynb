{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"v_fG538P6N4l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663506667187,"user_tz":-330,"elapsed":13,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"}},"outputId":"94f86dd2-fc17-4c01-a1a2-eb57f4200466"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Sep 18 13:11:18 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   60C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"iYAvgeVDRXYG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663506691655,"user_tz":-330,"elapsed":24474,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"}},"outputId":"8988f9a5-7436-4479-e9d8-17dc7cb02a0f"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"LLyFOsSoByip","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663506691656,"user_tz":-330,"elapsed":12,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"}},"outputId":"b8081026-b78b-4f21-fa25-4195cb837dc9"},"source":["cd '/content/gdrive/My Drive/EXP/AdaInject/CIFAR100/'"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/EXP/AdaInject/CIFAR100\n"]}]},{"cell_type":"code","metadata":{"id":"MY9oaZ5cQt9D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dafe5c22-d5c9-4c3b-e723-8ea54c222b1b","executionInfo":{"status":"ok","timestamp":1663506970684,"user_tz":-330,"elapsed":279036,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"}}},"source":["!python3 main_cifar100.py\n","#!python3 main_cifar100.py --resume --lr=0.001\n","#!python3 main_cifar100.py --resume --lr=0.0001"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n","==> Building model..\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","\n","Epoch: 0\n","/content/gdrive/My Drive/EXP/AdaInject/CIFAR100/AdamInject.py:71: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, dg)\n"," [================================================================>]  Step: 272ms | Tot: 30s637ms | Loss: 4.365 | Acc: 2.648% (1324/50000) 782/782 \n"," [================================================================>]  Step: 6ms | Tot: 2s571ms | Loss: 4.200 | Acc: 3.470% (347/10000) 157/157 \n","Saving..\n","\n","Epoch: 1\n"," [================================================================>]  Step: 26ms | Tot: 31s613ms | Loss: 3.977 | Acc: 6.362% (3181/50000) 782/782 \n"," [================================================================>]  Step: 5ms | Tot: 2s514ms | Loss: 3.697 | Acc: 10.520% (1052/10000) 157/157 \n","Saving..\n","\n","Epoch: 2\n"," [================================================================>]  Step: 25ms | Tot: 31s445ms | Loss: 3.550 | Acc: 12.286% (6143/50000) 782/782 \n"," [================================================================>]  Step: 6ms | Tot: 2s514ms | Loss: 3.382 | Acc: 14.940% (1494/10000) 157/157 \n","Saving..\n","\n","Epoch: 3\n"," [================================================================>]  Step: 25ms | Tot: 31s232ms | Loss: 3.209 | Acc: 17.492% (8746/50000) 782/782 \n"," [================================================================>]  Step: 5ms | Tot: 2s468ms | Loss: 3.060 | Acc: 20.240% (2024/10000) 157/157 \n","Saving..\n","\n","Epoch: 4\n"," [================================================================>]  Step: 26ms | Tot: 31s320ms | Loss: 2.915 | Acc: 23.118% (11559/50000) 782/782 \n"," [================================================================>]  Step: 5ms | Tot: 2s566ms | Loss: 2.905 | Acc: 24.600% (2460/10000) 157/157 \n","Saving..\n","\n","Epoch: 5\n"," [================================================================>]  Step: 26ms | Tot: 31s310ms | Loss: 2.672 | Acc: 28.190% (14095/50000) 782/782 \n"," [================================================================>]  Step: 5ms | Tot: 2s492ms | Loss: 2.505 | Acc: 32.350% (3235/10000) 157/157 \n","Saving..\n","\n","Epoch: 6\n"," [================================================================>]  Step: 25ms | Tot: 31s366ms | Loss: 2.448 | Acc: 33.226% (16613/50000) 782/782 \n"," [================================================================>]  Step: 6ms | Tot: 2s474ms | Loss: 2.322 | Acc: 36.530% (3653/10000) 157/157 \n","Saving..\n","\n","Epoch: 7\n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f351737ef80>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1474, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 140, in join\n","    res = self._popen.wait(timeout)\n","  File \"/usr/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n","    if not wait([self.sentinel], timeout):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n","    ready = selector.select(timeout)\n","  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n","    fd_event_list = self._selector.poll(timeout)\n","KeyboardInterrupt: \n","Traceback (most recent call last):\n","  File \"main_cifar100.py\", line 171, in <module>\n","    train(epoch)\n","  File \"main_cifar100.py\", line 104, in train\n","    optimizer.step()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n","    return wrapped(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\", line 113, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/content/gdrive/My Drive/EXP/AdaInject/CIFAR100/AdamInject.py\", line 72, in step\n","    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","KeyboardInterrupt\n"]}]}]}