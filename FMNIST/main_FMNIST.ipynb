{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"iYAvgeVDRXYG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663507811805,"user_tz":-330,"elapsed":5543,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"}},"outputId":"d757b63e-46be-441e-8344-7dc248c26d2a"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"LLyFOsSoByip","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663507811806,"user_tz":-330,"elapsed":16,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"}},"outputId":"b2abc453-188b-4233-a047-1e559e8534d0"},"source":["cd '/content/gdrive/My Drive/EXP/AdaInject/FMNIST/'"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/EXP/AdaInject/FMNIST\n"]}]},{"cell_type":"code","metadata":{"id":"v_fG538P6N4l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663507811807,"user_tz":-330,"elapsed":14,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"}},"outputId":"9ae53f69-08ca-44b3-9b89-0e60ee7e06b0"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Sep 18 13:30:24 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"MY9oaZ5cQt9D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9227f0b6-5740-4cc5-a665-0ccda4cb3d99","executionInfo":{"status":"ok","timestamp":1663507864222,"user_tz":-330,"elapsed":52422,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"}}},"source":["!python3 main_fmnist.py\n","#!python3 main_fmnist.py --resume --lr=0.001\n","#!python3 main_fmnist.py --resume --lr=0.0001"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Preparing data..\n","==> Building model..\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","\n","Epoch: 0\n","/content/gdrive/My Drive/EXP/AdaInject/FMNIST/AdamInject.py:71: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, dg)\n"," [================================================================>]  Step: 347ms | Tot: 37s290ms | Loss: 0.612 | Acc: 76.963% (46178/60000) 938/938 \n"," [================================================================>]  Step: 108ms | Tot: 2s866ms | Loss: 0.408 | Acc: 84.970% (8497/10000) 157/157 \n","Saving..\n","\n","Epoch: 1\n","Traceback (most recent call last):\n","  File \"main_fmnist.py\", line 179, in <module>\n","    train(epoch)\n","  File \"main_fmnist.py\", line 113, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 396, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n"]}]}]}