{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1663506071405,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"},"user_tz":-330},"id":"v_fG538P6N4l","outputId":"f181a958-bd00-409d-d41f-4af25507ada3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Sep 18 13:01:24 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   67C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5646,"status":"ok","timestamp":1663506077043,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"},"user_tz":-330},"id":"iYAvgeVDRXYG","outputId":"1eccaeca-bf76-4aa3-a64d-2a5a2d849dd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1663506077044,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"},"user_tz":-330},"id":"LLyFOsSoByip","outputId":"15361b15-374a-41ef-b482-91875a0e3eb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/EXP/AdaInject/CIFAR10\n"]}],"source":["cd '/content/gdrive/My Drive/EXP/AdaInject/CIFAR10/'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MY9oaZ5cQt9D","outputId":"2c08bb6d-42ee-4c0b-a475-763ebd47ec62","executionInfo":{"status":"ok","timestamp":1663506128448,"user_tz":-330,"elapsed":51411,"user":{"displayName":"Shiv Ram Dubey","userId":"03383190577487484960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n","==> Building model..\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","\n","Epoch: 0\n","/content/gdrive/My Drive/EXP/AdaInject/CIFAR10/AdaBeliefInject.py:124: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"," [================================================================>]  Step: 297ms | Tot: 33s77ms | Loss: 1.536 | Acc: 41.342% (20671/50000) 782/782 \n"," [================================================================>]  Step: 5ms | Tot: 2s554ms | Loss: 1.169 | Acc: 58.710% (5871/10000) 157/157 \n","Saving..\n","\n","Epoch: 1\n","Traceback (most recent call last):\n","  File \"main_cifar10.py\", line 165, in <module>\n","    train(epoch)\n","  File \"main_cifar10.py\", line 100, in train\n","    optimizer.step()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n","    return wrapped(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\", line 113, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/content/gdrive/My Drive/EXP/AdaInject/CIFAR10/AdaBeliefInject.py\", line 126, in step\n","    exp_avg.mul_(beta1).add_(1 - beta1, (grad + diff_data * grad * grad)/2)\n","KeyboardInterrupt\n"]}],"source":["!python3 main_cifar10.py\n","#!python3 main_cifar10.py --resume --lr=0.001\n","#!python3 main_cifar10.py --resume --lr=0.0001"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}